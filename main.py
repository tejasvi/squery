# -*- coding: utf-8 -*-
"""nb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tseefwrT-YfoRDatTTHGocO0115ryffD
"""

# Commented out IPython magic to ensure Python compatibility.
# !pip -q install gensim
# !pip -q install line-profiler
# %load_ext line_profiler

# !wget -qO "training data.xlsx" https://cdn.skillenza.com/files/6a5fa354-63f4-4075-ae9f-ed47b60c41c7/Train-Data.xlsx

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from itertools import chain
from collections import Counter
import regex as re
import pickle
from fuzzywuzzy import process

t = pd.read_excel("training data.xlsx")

t["wt"] = np.asarray(
    [
        int(float(re.sub("\D", "", str(x))))
        if re.sub("\D", "", str(x)) != ""
        else -99999
        for x in t["Grammage"].values
    ],
    dtype=int,
)

t.to_pickle("train.pkl")


def preproc(s):
    return [
        x
        for x in simple_preprocess(s)
        if x
        not in {
            "gm",
            "ml",
            "kg",
            "with",
            "for",
            "the",
            "rs",
            "of",
            "under",
            "less",
            "more",
            "than",
            "lower",
            "greater",
        }
    ]


desc = t["Product Description"].tolist()
dlist = []
for i, des in enumerate(desc):
    dlist.append(preproc(des))
with open('dlist.pkl', 'wb') as f:
    pickle.dump(dlist, f)

model = Word2Vec(dlist, size=50, window=15, min_count=1, workers=8, iter=50)
model.save("word2vec.model")

frequencies = Counter(list(chain.from_iterable(dlist)))
with open('freq.pkl', 'wb') as f:
    pickle.dump(frequencies, f)

def autocomplete(q, freq):
    return next((k for k in freq if k.startswith(q)), None)

def run_sif(query, sentences2, model, freqs={}, a=0.001, fuzz=3):
    total_freq = sum(freqs.values())
    embeddings = []

    tokens1 = []
        for token in query:
            if token in model.wv:
                tokens1.append(token)
            else:
                for i in process.extract(token, model.wv.vocab.keys(), limit=fuzz):
                    tokens1.append(i[0])
    if not tokens1:
        return None
    for i in range(tokens1.count(None)): tokens1.remove(None)
    weights1 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens1]
    embedding1 = np.zeros((len(sentences2), model.trainables.layer1_size)) + np.average(
        [model.wv[token] for token in tokens1], axis=0, weights=weights1
    )

    embedding2 = np.zeros((len(sentences2), model.trainables.layer1_size))

    # SIF requires us to first collect all sentence embeddings and then perform
    # common component analysis.
    for i, sent2 in enumerate(sentences2):

        tokens2 = [token for token in sent2 if token in model.wv]
        n = len(set(tokens1) & set(tokens2)) / len(tokens1)

        weights2 = [a / (a + freqs.get(token, 0) / total_freq) for token in tokens2]
        embedding2[i] = np.average(
            [model.wv[token] for token in tokens2], axis=0, weights=weights2
        )

        embedding1[i] += 15 * n * embedding2[i]

    sims = np.einsum("ij,ij->i", embedding1, embedding2) / (
        np.linalg.norm(embedding1, axis=1) * np.linalg.norm(embedding2, axis=1)
    )

    return sims


def wt(q):
    w = i = None
    for i, x in enumerate(q):
        if x in ("gm", "ml", "kg", "g", "l", "lt", "ltr", "ml", "pcs", "xgm",):
            if i:
                try:
                    w = int(float(q[i - 1]))
                except:
                    pass
    return w


def cost(q):
    more = c = i = None
    for i, x in enumerate(q):
        if x in ("less", "lower"):
            if i < len(q) - 2:
                try:
                    c = int(float(q[i + 2]))
                    more = 1
                except:
                    pass
        elif x == "under":
            if i < len(q) - 1:
                try:
                    c = int(float(q[i + 1]))
                    more = 1
                except:
                    pass
        elif x in ("more", "greater"):
            if i < len(q) - 2:
                try:
                    c = int(float(q[i + 2]))
                    more = 3
                except:
                    pass
        if x == "rs":
            if i:
                try:
                    c = int(float(q[i - 1]))
                    more = 2
                except:
                    pass
            else:
                try:
                    c = int(float(q[i + 1]))
                    more = 2
                except:
                    pass
        if c:
            break
    return more, c


def run(q, boost=[], b=1, n=10, fuzz=3):
    qcheck = re.sub(r"([0-9]+(\.[0-9]+)?)", r" \1 ", q.lower()).strip().split()
    grammage = wt(qcheck)
    op, price = cost(qcheck)

    q += 4 * int(b) * (" " + " ".join(boost))
    scores = run_sif(preproc(q), dlist, freqs=frequencies, model=model, fuzz=fuzz)
    df = t.copy()
    df["scores"] = scores

    # price
    if price:
        if op == 1:
            df.loc[df["Final Price"] < price, "scores"] += 0.005
        elif op == 2:
            df.loc[df["Final Price"].between(price - 10, price + 10), "scores"] += 0.005
        elif op == 3:
            df.loc[df["Final Price"] > price, "scores"] += 0.005

    # grammage
    if grammage:
        df.loc[df["wt"] == grammage, "scores"] += 0.005

    return df


df = run("powder 250 gm 150 rs", boost=["ayghd"], fuzz=3)
df.sort_values("scores", ascending=False)[
    ["Product Description", "Grammage", "Final Price", "scores"]
].head(10)

# Commented out IPython magic to ensure Python compatibility.
# %timeit run("powder 250 gm 150 rs", boost=["ayghd"])

# Commented out IPython magic to ensure Python compatibility.
# %lprun -f run df=run("powder 250 gm 150 rs", boost=["ayghd"])
